{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d78fb079-5f61-437a-a4fd-b103de169504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "#accessing the zip file downloaded using Kaggle API\n",
    "file_path = \"/Users/papayaw/projects/Walmart Retail Pipeline/walmart-dataset.zip\"\n",
    "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"walmart_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c0d64ff1-a4ca-4a82-a8b4-e210244dedc1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 88,
    "lastExecutedAt": 1745423182500,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nimport os\n\n# Extract function is already implemented for you \ndef extract(store_data, extra_data):\n    extra_df = pd.read_parquet(extra_data)\n    merged_df = store_data.merge(extra_df, on = \"index\")\n    return merged_df\n\n# Call the extract() function and store it as the \"merged_df\" variable\nmerged_df = extract(grocery_sales, \"extra_data.parquet\")"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    " \n",
    "def extract(store_data):\n",
    "    \"\"\"\n",
    "    The goal of this function is to extract the dataset from the file.\n",
    "    \n",
    "    :param store_data: the file path where the grocery sales data is stored.\n",
    "    :return: a dataset containing sales data.\n",
    "    \"\"\"\n",
    "    store_df = pd.read_csv(store_data)\n",
    "\n",
    "    return store_df\n",
    "\n",
    "# Call the extract() function and store it as the \"sales_df\" variable\n",
    "sales_df = extract(\"walmart_data/walmart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6d3c25e2-e7d8-4c33-9be0-d45f03b2cf43",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1745423182554,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the transform() function with one parameter: \"raw_data\"\nimport numpy as np\nimport datetime\n\ndef transform(raw_data):\n\n    #filling missing values with the mean\n    mean_weekly_sales = np.mean(raw_data['Weekly_Sales'])\n    mean_CPI = np.mean(raw_data['CPI'])\n    mean_unemployment = np.mean(raw_data['Unemployment'])\n    #applying mean\n    raw_data['Weekly_Sales'] = raw_data['Weekly_Sales'].fillna(mean_weekly_sales)\n    raw_data['CPI'] = raw_data['CPI'].fillna(mean_CPI)\n    raw_data['Unemployment'] = raw_data['Unemployment'].fillna(mean_unemployment)\n    #dropping null values for the date column\n    raw_data['Date'] = raw_data['Date'].dropna()\n    #adding month column\n    raw_data['Month'] = raw_data['Date'].dt.month\n    #keeping rows where weekly_sales>10000\n    raw_data = raw_data[raw_data['Weekly_Sales']>10000]\n    #selecting the relevant columns \n    relevant_columns = ['Store_ID','Month','Dept','IsHoliday','Weekly_Sales','CPI','Unemployment']\n    clean_df = raw_data.loc[:,relevant_columns]\n\n    return clean_df\npass"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "def transform(raw_data):\n",
    "    \"\"\"\n",
    "    To prepare the data for analysis, a number of transformations \n",
    "    will be applied:\n",
    "        -filling missing values of columns 'Weekly_Sales', 'CPI', and 'Unemployment' with their mean.\n",
    "        -transform the 'Date' column to the datetime dtype.\n",
    "        - dropping null values in the 'Date' column.\n",
    "        -creating the 'Month' column using data in the 'Date' column.\n",
    "        -creating the 'Year' column using data in the 'Date' column.\n",
    "        - selecting rows where 'Weekly_Sales' is greater than 10,000.\n",
    "        - selecting the desired columns for analysis.\n",
    "    \n",
    "    :param raw_data: the sales dataframe.\n",
    "    :return: a cleaned dataset having had the operations above applied to it\n",
    "    \"\"\"\n",
    "\n",
    "    #filling missing values with the mean\n",
    "    mean_weekly_sales = np.mean(raw_data['Weekly_Sales'])\n",
    "    mean_CPI = np.mean(raw_data['CPI'])\n",
    "    mean_unemployment = np.mean(raw_data['Unemployment'])\n",
    "    #applying mean\n",
    "    raw_data['Weekly_Sales'] = raw_data['Weekly_Sales'].fillna(mean_weekly_sales)\n",
    "    raw_data['CPI'] = raw_data['CPI'].fillna(mean_CPI)\n",
    "    raw_data['Unemployment'] = raw_data['Unemployment'].fillna(mean_unemployment)\n",
    "    #changing 'Date' dtype to datetime\n",
    "    raw_data['Date'] = pd.to_datetime(raw_data['Date'],format='%d-%m-%Y')\n",
    "    #dropping null values for the date column\n",
    "    raw_data['Date'] = raw_data['Date'].dropna()\n",
    "    #adding month column\n",
    "    raw_data['Month'] = raw_data['Date'].dt.month\n",
    "    #adding the year column\n",
    "    raw_data['Year'] = raw_data['Date'].dt.year\n",
    "    #keeping rows where weekly_sales>10000\n",
    "    raw_data = raw_data[raw_data['Weekly_Sales']>10000]\n",
    "    #selecting the relevant columns \n",
    "    relevant_columns = ['Store','Month', 'Year','Holiday_Flag','Weekly_Sales','CPI','Unemployment']\n",
    "    clean_df = raw_data.loc[:,relevant_columns]\n",
    "\n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "620b7289-06cd-4205-be9e-a50dc8d36cf0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1745423182606,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the transform() function and pass the merged DataFrame\nclean_data = transform(merged_df)",
    "outputsMetadata": {
     "0": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "546e8825-838b-443d-8473-bca29f3df4a9",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Call the transform() function and pass the merged DataFrame\n",
    "clean_data = transform(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b19b15e3-6624-47a9-927f-d3f12fe8212d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1745423182654,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\ndef avg_weekly_sales_per_month(clean_data):\n    # grouping month and weekly_sales columns\n    group_df = clean_data.groupby('Month')\n    new_data = group_df['Weekly_Sales'].agg('mean').round(2)\n    new_data = new_data.reset_index()\n    new_data = new_data.rename(columns={'Weekly_Sales':'Avg_Sales'})\n    \n\n    return new_data\n    pass"
   },
   "outputs": [],
   "source": [
    "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "    \n",
    "    # grouping month and weekly_sales columns\n",
    "    group_df = clean_data.groupby('Month')\n",
    "    new_data = group_df['Weekly_Sales'].agg('mean').round(2)\n",
    "    new_data = new_data.reset_index()\n",
    "    new_data = new_data.rename(columns={'Weekly_Sales':'Avg_Sales'})\n",
    "    \n",
    "\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fe875e27-b0cf-4e52-994e-4ae1fe6e8876",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1745423182706,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\nagg_data = avg_weekly_sales_per_month(clean_data)",
    "outputsMetadata": {
     "0": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "546e8825-838b-443d-8473-bca29f3df4a9",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n",
    "agg_data = avg_weekly_sales_per_month(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "921cb123-3153-4334-bdeb-9bb227fdc530",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1745423182754,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n    full_data.to_csv('clean_data.csv', index=False)\n    agg_data.to_csv('agg_data.csv', index=False)\n    pass"
   },
   "outputs": [],
   "source": [
    "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "    full_data.to_csv('clean_data.csv', index=False)\n",
    "    agg_data.to_csv('agg_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f518ad5c-214e-474b-80bd-827b0c0e1536",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 393,
    "lastExecutedAt": 1745423183147,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \nload(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')"
   },
   "outputs": [],
   "source": [
    "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths    \n",
    "load(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "61b5f58a-70cb-40b3-bdbe-20b4079276e3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 46,
    "lastExecutedAt": 1745423183194,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\nimport os\n\ndef validation(file_path):\n    if os.path.exists(file_path):\n        print('Exists')\n    else:\n        raise Exception(f\"File not found at path {file_path}\")\n    pass"
   },
   "outputs": [],
   "source": [
    "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "import os\n",
    "\n",
    "def validation(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print('Exists')\n",
    "    else:\n",
    "        raise Exception(f\"File not found at path {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df1659ff-41c4-4a92-9812-80c6eaa02b90",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1745423183242,
    "lastExecutedByKernel": "954f8f70-f01d-454c-b558-732bc1c450ac",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation('clean_data.csv')\nvalidation('agg_data.csv')",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists\n",
      "Exists\n"
     ]
    }
   ],
   "source": [
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation('clean_data.csv')\n",
    "validation('agg_data.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
